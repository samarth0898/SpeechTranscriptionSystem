{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "import glob\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the various characters in the transcripts of the datasetW\n",
    "VOCAB = ['<sos>',   \n",
    "         'A',   'B',    'C',    'D',    \n",
    "         'E',   'F',    'G',    'H',    \n",
    "         'I',   'J',    'K',    'L',       \n",
    "         'M',   'N',    'O',    'P',    \n",
    "         'Q',   'R',    'S',    'T', \n",
    "         'U',   'V',    'W',    'X', \n",
    "         'Y',   'Z',    \"'\",    ' ', \n",
    "         '<eos>']\n",
    "\n",
    "VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n",
    "\n",
    "SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n",
    "EOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n",
    "\n",
    "BATCH_SIZE = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCDataset:\n",
    "    def __init__(self, data_path, vocab_map, val = False, cep_norm = True):\n",
    "        \"\"\"\n",
    "        Let's access the datapaths for the input and the labels in this sections \n",
    "        x: MFCC path\n",
    "        y: Transcripts \n",
    "\n",
    "        1) Load all the data a-priori in the init for faster training. \n",
    "        2) Cepstral normalization :  \n",
    "        \"\"\"\n",
    "        self.val = val\n",
    "        self.cep_norm = cep_norm \n",
    "        if self.val:\n",
    "            self.x =  str(data_path)+\"\\\\dev-clean\\\\mfcc\\\\*.npy\" \n",
    "            self.y =  str(data_path)+\"\\\\dev-clean\\\\transcript\\\\*.npy\"\n",
    "        else: \n",
    "            self.x = str(data_path)+\"\\\\train-clean-100\\\\mfcc\\\\*.npy\"\n",
    "            self.y = str(data_path)+\"\\\\train-clean-100\\\\transcript\\\\raw\\\\*.npy\"\n",
    "\n",
    "        self.mfcc_list = sorted(glob.glob(self.x))\n",
    "        self.transcript_list = sorted(glob.glob(self.y))\n",
    "        self.alphabets = vocab_map\n",
    "       \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.mfcc_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        cepstral normalization performed here for higher SNR \n",
    "        \"\"\"\n",
    "\n",
    "        if self.val:\n",
    "            mf_temp = np.load(self.mfcc_list[index], allow_pickle= True)\n",
    "            tr_temp = np.load(self.transcript_list[index], allow_pickle= True)\n",
    "            tr_temp = [self.alphabets[ele] for ele in tr_temp]\n",
    "            if self.cep_norm:\n",
    "                mf_temp = (mf_temp - np.mean(mf_temp, axis = 0))/ np.std(mf_temp)\n",
    "            \n",
    "            return torch.tensor(mf_temp)\n",
    "        \n",
    "        else: \n",
    "            mf_temp = np.load(self.mfcc_list[index], allow_pickle= True)\n",
    "            tr_temp = np.load(self.transcript_list[index], allow_pickle= True)\n",
    "            \n",
    "            # Converting the alphabets in the labels to integers using the pre-defined map provided \n",
    "            tr_temp = [self.alphabets[ele] for ele in tr_temp]\n",
    "\n",
    "            if self.cep_norm:\n",
    "                mf_temp = (mf_temp - np.mean(mf_temp, axis = 0))/ np.std(mf_temp)\n",
    "            return torch.tensor(mf_temp), torch.tensor(tr_temp)\n",
    "\n",
    "#Collate function for uniform padding of the input sequences \n",
    "\n",
    "def collate_train_val(data): \n",
    "    \n",
    "    (xx, yy) = zip(*data)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "\n",
    "    xx_pad = pad_sequence(xx,batch_first=True)\n",
    "    yy_pad = pad_sequence(yy,batch_first=True)\n",
    "\n",
    "    x_lens = np.asarray(x_lens)\n",
    "    y_lens = np.asarray(y_lens)\n",
    "    # Some augmentation and masking here may help the network converge better. \n",
    "\n",
    "        \n",
    "    return xx_pad, yy_pad, torch.tensor(x_lens), torch.tensor(y_lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and dataloader sections \n",
    "data_path = \"C:\\\\Users\\\\thopa\\Desktop\\\\Assignments\\\\11685\\\\HW4\\\\2022Implementation\\\\11-785-f22-hw4p2\\\\hw4p2\"\n",
    "train_data = MFCCDataset(data_path, vocab_map= VOCAB_MAP)\n",
    "val_data = MFCCDataset(data_path,vocab_map= VOCAB_MAP, val = True)\n",
    "\n",
    "train_loader = DataLoader(train_data , collate_fn= collate_train_val , shuffle = True, pin_memory= True)\n",
    "val_loader = DataLoader(val_data, collate_fn = collate_train_val, shuffle= False, pin_memory = True)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network and Training "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silly notes for reference \n",
    "When training RNN (LSTM or GRU or vanilla-RNN), it is difficult to batch the variable length sequences. For example: if the length of sequences in a size 8 batch is [4,6,8,5,4,3,7,8], you will pad all the sequences and that will result in 8 sequences of length 8. You would end up doing 64 computations (8x8), but you needed to do only 45 computations. Moreover, if you wanted to do something fancy like using a bidirectional-RNN, it would be harder to do batch computations just by padding and you might end up doing more computations than required.\n",
    "\n",
    "Instead, PyTorch allows us to pack the sequence, internally packed sequence is a tuple of two lists. One contains the elements of sequences. Elements are interleaved by time steps (see example below) and other contains the size of each sequence the batch size at each step. This is helpful in recovering the actual sequences as well as telling RNN what is the batch size at each time step. This has been pointed by @Aerin. This can be passed to RNN and it will internally optimize the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(PBLSTM,self).__init__()\n",
    "\n",
    "        self.blstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = 2, batch_first = True, bidirectional = True, dropout = 0.3)\n",
    "    \n",
    "    def reshape(self, x, x_lens):\n",
    "        # Reshaping for concatenation / reducing dimensions\n",
    "        batch, rows, cols = x.shape[0], x.shape[1], x.shape[2]\n",
    "\n",
    "        if (rows % 2 != 0):\n",
    "            x = x[:,:-1,:]\n",
    "        x = x.reshape(batch, int(rows/2), cols*2)\n",
    "        x_lens = x_lens//2\n",
    "    \n",
    "        return x, x_lens\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Computational savings and original sequence recovery using the pack padded and pad packed routine \n",
    "        \"\"\"\n",
    "        x_pad, x_pad_lens = pad_packed_sequence(x, batch_first=True)\n",
    "        x, x_lens = self.reshape(x_pad, x_pad_lens.to(\"cuda\"))\n",
    "        input = rnn_utils.pack_padded_sequence(x, lengths = x_lens.cpu(), batch_first= True, enforce_sorted= False)\n",
    "        rnn_out, _ = self.blstm(input)\n",
    "        #output, lens = rnn_utils.pad_packed_sequence(rnn_out, batch_first= True)\n",
    "        \n",
    "        return rnn_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, encoder_hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \"\"\"\n",
    "        The encoder used is a pyramidal-BiLSTM for matching the input rate and the speech transcription rate which is about 8:1. \n",
    "        This model is sigificantly influenced by the LAS paper \n",
    "        LAS: Chan, William, et al. \"Listen, attend and spell.\" arXiv preprint arXiv:1508.01211 (2015).\n",
    "\n",
    "        [REF: B. Raj, Deep Learning Carnegie Mellon University]\n",
    "        The pBLSTM is a variant of Bi-LSTMs that downsamples sequences by a factor of 2 by concatenating\n",
    "        adjacent pairs of inputs before running a conventional Bi-LSTM on the reduced-length sequence. So, given\n",
    "        an input vector sequence X0, X1, X2, X3, . . . XN−1, the pBLSTM first concatenates adjacent pairs of vectors\n",
    "        as [X0, X1], [X2, X3], . . . [XN−2, XN−1], and then computes a regular BiLSTM on the reshaped input.\n",
    "\n",
    "        -) Initial Bi-LSTM \n",
    "        -) 3x Pyramidal Bi-LSTM \n",
    "        \"\"\"    \n",
    "        self.base_lstm = nn.LSTM(input_size = input_size, hidden_size = encoder_hidden_size, num_layers = 1, batch_first = True, bidirectional = True, dropout = 0.1)\n",
    "        self.pblstm = nn.Sequential(PBLSTM(4*encoder_hidden_size,encoder_hidden_size),PBLSTM(4*encoder_hidden_size,encoder_hidden_size),PBLSTM(4*encoder_hidden_size,encoder_hidden_size))\n",
    "        \n",
    "    def forward(self, x, x_lens):\n",
    "        pack_padd_out = pack_padded_sequence(x, x_lens.to('cpu'),batch_first=True, enforce_sorted=False)\n",
    "        #print(type(pack_padd_out))\n",
    "        out_lstm, _  = self.base_lstm(pack_padd_out)\n",
    "        encoder_outputs = self.pblstm(out_lstm)\n",
    "        encoder_outputs, encoder_lens = pad_packed_sequence(encoder_outputs, batch_first=True)\n",
    "\n",
    "        return encoder_outputs, encoder_lens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 1282, 15]) torch.Size([1, 188]) torch.Size([1]) torch.Size([1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\PointNet\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n",
      "C:\\Users\\thopa\\AppData\\Local\\Temp\\ipykernel_16628\\2177239868.py:14: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  x_lens = x_lens//2\n"
     ]
    }
   ],
   "source": [
    "from torchsummaryX import summary\n",
    "\n",
    "for data in train_loader:\n",
    "    x, y, lx, ly = data\n",
    "    print(x.shape, y.shape, lx.shape, ly.shape)\n",
    "    break \n",
    "\n",
    "encoder = Encoder(15,256)# TODO: Initialize Listener\n",
    "out, lens = encoder.forward(x, lx)\n",
    "del encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (904003134.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn [56], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    class Attention(nn.Module):\u001b[0m\n\u001b[1;37m                               ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "# Attention block \n",
    "\"\"\"\n",
    "Possible Efficiencies with the attention mechanism (d2l book)\n",
    "1) In general, it requires that both the query and the key have the \n",
    "same vector length, say d, even though this can be addressed easily by replacing \n",
    "q⊤k with q⊤Mk where M is a suitably chosen matrix to translate\n",
    "between both spaces. For now assume that the dimensions match.\n",
    "2) Adding dropout weights also helps \n",
    "\"\"\"\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Attention, self).__init__()\n",
    "        \n",
    "        \n",
    "    def forward(self, query, key, value, mask):\n",
    "        energy = torch.bmm(key, query.unsqueeze(2))\n",
    "        energy = torch.squeeze(energy, dim = 2)\n",
    "\n",
    "        #What should the mask least value be? \n",
    "        energy.masked_fill_(mask, -1e9)\n",
    "\n",
    "        attention = torch.nn.functional.softmax(energy, dim = 1)\n",
    "        context = torch.bmm(attention.unsqueeze(1), value)\n",
    "        \n",
    "        return context, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Listener(15,256)# TODO: Initialize Listener\n",
    "encoder = encoder.to(DEVICE)\n",
    "print(encoder)\n",
    "summary(encoder, x.to(DEVICE), lx)\n",
    "del encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PointNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be71f7ebca6fc406e8f2b7eb97fc71d65ed284e191505e45141d8f3008802363"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
