{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\PointNet\\lib\\site-packages\\scipy\\__init__.py:146: UserWarning: A NumPy version >=1.16.5 and <1.23.0 is required for this version of SciPy (detected version 1.23.3\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import torch.optim as optim\n",
    "import torch.nn.utils as utils\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import random\n",
    "from torch.utils import data\n",
    "from torch.nn.utils.rnn import pad_sequence, pack_padded_sequence, pad_packed_sequence\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from typing import List, Tuple, Dict\n",
    "import os\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "\n",
    "import Levenshtein\n",
    "import torchaudio\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# These are the various characters in the transcripts of the datasetW\n",
    "VOCAB = ['<sos>',   \n",
    "         'A',   'B',    'C',    'D',    \n",
    "         'E',   'F',    'G',    'H',    \n",
    "         'I',   'J',    'K',    'L',       \n",
    "         'M',   'N',    'O',    'P',    \n",
    "         'Q',   'R',    'S',    'T', \n",
    "         'U',   'V',    'W',    'X', \n",
    "         'Y',   'Z',    \"'\",    ' ', \n",
    "         '<eos>']\n",
    "\n",
    "VOCAB_MAP = {VOCAB[i]:i for i in range(0, len(VOCAB))}\n",
    "\n",
    "SOS_TOKEN = VOCAB_MAP[\"<sos>\"]\n",
    "EOS_TOKEN = VOCAB_MAP[\"<eos>\"]\n",
    "\n",
    "BATCH_SIZE = 96"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFCCDataset:\n",
    "    def __init__(self, data_path, vocab_map, val = False, cep_norm = True):\n",
    "        \"\"\"\n",
    "        Let's access the datapaths for the input and the labels in this sections \n",
    "        x: MFCC path\n",
    "        y: Transcripts \n",
    "\n",
    "        1) Load all the data a-priori in the init for faster training. \n",
    "        2) Cepstral normalization :  \n",
    "        \"\"\"\n",
    "        self.val = val\n",
    "        self.cep_norm = cep_norm \n",
    "        if self.val:\n",
    "            self.x =  str(data_path)+\"\\\\dev-clean\\\\mfcc\\\\*.npy\" \n",
    "            self.y =  str(data_path)+\"\\\\dev-clean\\\\transcript\\\\raw\\\\*.npy\"\n",
    "        else: \n",
    "            self.x = str(data_path)+\"\\\\train-clean-100\\\\mfcc\\\\*.npy\"\n",
    "            self.y = str(data_path)+\"\\\\train-clean-100\\\\transcript\\\\raw\\\\*.npy\"\n",
    "\n",
    "        \n",
    "\n",
    "        self.mfcc_list = sorted(glob.glob(self.x))[:100]\n",
    "        self.transcript_list = sorted(glob.glob(self.y))[:100]\n",
    "        self.alphabets = vocab_map\n",
    "       \n",
    "    def __len__(self):\n",
    "        \n",
    "        return len(self.mfcc_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        \"\"\"\n",
    "        cepstral normalization performed here for higher SNR \n",
    "        \"\"\"\n",
    "\n",
    "        if self.val:\n",
    "            mf_temp = np.load(self.mfcc_list[index], allow_pickle= True)\n",
    "            tr_temp = np.load(self.transcript_list[index], allow_pickle= True)\n",
    "            tr_temp = [self.alphabets[ele] for ele in tr_temp]\n",
    "            if self.cep_norm:\n",
    "                mf_temp = (mf_temp - np.mean(mf_temp, axis = 0))/ np.std(mf_temp)\n",
    "            \n",
    "            return torch.tensor(mf_temp),torch.tensor(tr_temp)\n",
    "        \n",
    "        else: \n",
    "            mf_temp = np.load(self.mfcc_list[index], allow_pickle= True)\n",
    "            tr_temp = np.load(self.transcript_list[index], allow_pickle= True)\n",
    "            \n",
    "            # Converting the alphabets in the labels to integers using the pre-defined map provided \n",
    "            tr_temp = [self.alphabets[ele] for ele in tr_temp]\n",
    "\n",
    "            if self.cep_norm:\n",
    "                mf_temp = (mf_temp - np.mean(mf_temp, axis = 0))/ np.std(mf_temp)\n",
    "            return torch.tensor(mf_temp), torch.tensor(tr_temp)\n",
    "\n",
    "#Collate function for uniform padding of the input sequences \n",
    "def collate_train(data): \n",
    "    \n",
    "    time_mask = torchaudio.transforms.TimeMasking(80)\n",
    "    frequency_mask =torchaudio.transforms.FrequencyMasking(5)\n",
    "    \n",
    "\n",
    "    (xx, yy) = zip(*data)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "\n",
    "    xx_pad = pad_sequence(xx,batch_first=True)\n",
    "    yy_pad = pad_sequence(yy,batch_first=True)\n",
    "    batch_mfcc_pad = np.transpose(xx_pad,(0,2,1))\n",
    "    batch_mfcc_pad = time_mask(xx_pad)\n",
    "    batch_mfcc_pad = frequency_mask(xx_pad)\n",
    "    batch_mfcc_pad = np.transpose(xx_pad,(0,2,1))\n",
    "    x_lens = np.asarray(x_lens)\n",
    "    y_lens = np.asarray(y_lens)\n",
    "    # Some augmentation and masking here may help the network converge better. \n",
    "\n",
    "        \n",
    "    return xx_pad, yy_pad, torch.tensor(x_lens), torch.tensor(y_lens)\n",
    "\n",
    "def collate_val(data): \n",
    "    \n",
    "    (xx, yy) = zip(*data)\n",
    "    x_lens = [len(x) for x in xx]\n",
    "    y_lens = [len(y) for y in yy]\n",
    "\n",
    "    xx_pad = pad_sequence(xx,batch_first=True)\n",
    "    yy_pad = pad_sequence(yy,batch_first=True)\n",
    "\n",
    "    x_lens = np.asarray(x_lens)\n",
    "    y_lens = np.asarray(y_lens)\n",
    "    # Some augmentation and masking here may help the network converge better. \n",
    "\n",
    "        \n",
    "    return xx_pad, yy_pad, torch.tensor(x_lens), torch.tensor(y_lens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset and dataloader sections \n",
    "data_path = \"C:\\\\Users\\\\thopa\\Desktop\\\\Assignments\\\\11685\\\\HW4\\\\2022Implementation\\\\11-785-f22-hw4p2\\\\hw4p2\"\n",
    "train_data = MFCCDataset(data_path, vocab_map= VOCAB_MAP)\n",
    "val_data = MFCCDataset(data_path,vocab_map= VOCAB_MAP, val = True)\n",
    "\n",
    "train_loader = DataLoader(train_data ,batch_size = 8 , collate_fn= collate_train , shuffle = True)\n",
    "val_loader = DataLoader(val_data, batch_size = 8, collate_fn = collate_val, shuffle= False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n"
     ]
    }
   ],
   "source": [
    "for i, (x,y,lx,ly) in enumerate(val_loader):\n",
    "    print(i)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network and Training "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Silly notes for reference \n",
    "When training RNN (LSTM or GRU or vanilla-RNN), it is difficult to batch the variable length sequences. For example: if the length of sequences in a size 8 batch is [4,6,8,5,4,3,7,8], you will pad all the sequences and that will result in 8 sequences of length 8. You would end up doing 64 computations (8x8), but you needed to do only 45 computations. Moreover, if you wanted to do something fancy like using a bidirectional-RNN, it would be harder to do batch computations just by padding and you might end up doing more computations than required.\n",
    "\n",
    "Instead, PyTorch allows us to pack the sequence, internally packed sequence is a tuple of two lists. One contains the elements of sequences. Elements are interleaved by time steps (see example below) and other contains the size of each sequence the batch size at each step. This is helpful in recovering the actual sequences as well as telling RNN what is the batch size at each time step. This has been pointed by @Aerin. This can be passed to RNN and it will internally optimize the computations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PBLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super(PBLSTM,self).__init__()\n",
    "\n",
    "        self.blstm = nn.LSTM(input_size = input_size, hidden_size = hidden_size, num_layers = 2, batch_first = True, bidirectional = True, dropout = 0.3)\n",
    "    \n",
    "    def reshape(self, x, x_lens):\n",
    "        # Reshaping for concatenation / reducing dimensions\n",
    "        batch, rows, cols = x.shape[0], x.shape[1], x.shape[2]\n",
    "\n",
    "        if (rows % 2 != 0):\n",
    "            x = x[:,:-1,:]\n",
    "        x = x.reshape(batch, int(rows/2), cols*2)\n",
    "        x_lens = x_lens//2\n",
    "    \n",
    "        return x, x_lens\n",
    "    \n",
    "    def forward(self,x):\n",
    "        \"\"\"\n",
    "        Computational savings and original sequence recovery using the pack padded and pad packed routine \n",
    "        \"\"\"\n",
    "        x_pad, x_pad_lens = pad_packed_sequence(x, batch_first=True)\n",
    "        x, x_lens = self.reshape(x_pad, x_pad_lens.to(\"cuda\"))\n",
    "        input = rnn_utils.pack_padded_sequence(x, lengths = x_lens.cpu(), batch_first= True, enforce_sorted= False)\n",
    "        rnn_out, _ = self.blstm(input)\n",
    "        #output, lens = rnn_utils.pad_packed_sequence(rnn_out, batch_first= True)\n",
    "        \n",
    "        return rnn_out"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Locked Dropout Regularization \n",
    "LockedDropout can be used to apply the same dropout mask to every time step. \n",
    "\n",
    "Benefits of locked dropout: \n",
    "- Reduces variance like any typical regularization process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LockedDropout(nn.Module):\n",
    "    def __init__(self, prob):\n",
    "        super(LockedDropout, self).__init__()\n",
    "        self.prob = prob \n",
    "    \n",
    "    def forward(self, x):\n",
    "        output, output_len = pad_packed_sequence(x, batch_first= True)\n",
    "        x = output\n",
    "        x = x.clone()\n",
    "        mask = x.new_empty(1, x.size(1), x.size(2), requires_grad= False).bernoulli(1- self.prob)\n",
    "        mask = mask.div_(1 - self.prob)\n",
    "        mask = mask.expand_as(x)\n",
    "        x_masked = x * mask \n",
    "        x_masked = pack_padded_sequence(x_masked, output_len.cpu(), batch_first= True,enforce_sorted=False)\n",
    "        return x_masked\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_size, encoder_hidden_size):\n",
    "        super(Encoder, self).__init__()\n",
    "        \"\"\"\n",
    "        The encoder used is a pyramidal-BiLSTM for matching the input rate and the speech transcription rate which is about 8:1. \n",
    "        This model is sigificantly influenced by the LAS paper \n",
    "        LAS: Chan, William, et al. \"Listen, attend and spell.\" arXiv preprint arXiv:1508.01211 (2015).\n",
    "\n",
    "        [REF: B. Raj, Deep Learning Carnegie Mellon University]\n",
    "        The pBLSTM is a variant of Bi-LSTMs that downsamples sequences by a factor of 2 by concatenating\n",
    "        adjacent pairs of inputs before running a conventional Bi-LSTM on the reduced-length sequence. So, given\n",
    "        an input vector sequence X0, X1, X2, X3, . . . XN−1, the pBLSTM first concatenates adjacent pairs of vectors\n",
    "        as [X0, X1], [X2, X3], . . . [XN−2, XN−1], and then computes a regular BiLSTM on the reshaped input.\n",
    "\n",
    "        -) Initial Bi-LSTM \n",
    "        -) 3x Pyramidal Bi-LSTM \n",
    "        -) Locked dropout regularization : \n",
    "        \n",
    "        \"\"\"    \n",
    "        self.base_lstm = nn.LSTM(input_size = input_size, hidden_size = encoder_hidden_size, num_layers = 1, batch_first = True, bidirectional = True, dropout = 0.1)\n",
    "        self.pblstm = nn.Sequential(PBLSTM(4*encoder_hidden_size,encoder_hidden_size), LockedDropout(0.25), PBLSTM(4*encoder_hidden_size,encoder_hidden_size), LockedDropout(0.25), PBLSTM(4*encoder_hidden_size,encoder_hidden_size), LockedDropout(0.25))\n",
    "        \n",
    "    def forward(self, x, x_lens):\n",
    "        pack_padd_out = pack_padded_sequence(x, x_lens.to('cpu'),batch_first=True, enforce_sorted=False)\n",
    "        #print(type(pack_padd_out))\n",
    "        out_lstm, _  = self.base_lstm(pack_padd_out)\n",
    "        encoder_outputs = self.pblstm(out_lstm)\n",
    "        encoder_outputs, encoder_lens = pad_packed_sequence(encoder_outputs, batch_first=True)\n",
    "\n",
    "        return encoder_outputs, encoder_lens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from torchsummaryX import summary\n",
    "\n",
    "# for data in train_loader:\n",
    "#     x, y, lx, ly = data\n",
    "#     print(x.shape, y.shape, lx.shape, ly.shape)\n",
    "#     break \n",
    "\n",
    "# encoder = Encoder(15,256)# TODO: Initialize Listener\n",
    "# out, lens = encoder.forward(x, lx)\n",
    "# del encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention block \n",
    "\"\"\"\n",
    "Possible Efficiencies with the attention mechanism (d2l book)\n",
    "1) In general, it requires that both the query and the key have the \n",
    "same vector length, say d, even though this can be addressed easily by replacing \n",
    "q⊤k with q⊤Mk where M is a suitably chosen matrix to translate\n",
    "between both spaces. For now assume that the dimensions match.\n",
    "2) Adding dropout weights also helps \n",
    "\"\"\"\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, encoder_output_size, decoder_output_size, projection):\n",
    "        super(Attention, self).__init__()\n",
    "        self.key_layer = nn.Linear(encoder_output_size, projection)\n",
    "        self.value_layer = nn.Linear(encoder_output_size, projection)\n",
    "        self.query_layer = nn.Linear(decoder_output_size, projection)\n",
    "    \n",
    "    def key_value_calc(self, encoder_output, encoder_len):\n",
    "        _ ,encoder_max_seq_len, _ = encoder_output.shape \n",
    "        self.key = self.key_layer(encoder_output)\n",
    "        self.value = self.value_layer(encoder_output)\n",
    "        # Attention mask \n",
    "        # Removing the influence of padding in the raw weights, we create a boolean mask of (batchsize, timesteps)\n",
    "        self.mask = (torch.arange(encoder_max_seq_len)[None, :] < encoder_len[:, None]).to(\"cuda\")\n",
    "        \n",
    "    def forward(self, decoder_output_embeddings):\n",
    "        self.query = self.query_layer(decoder_output_embeddings)\n",
    "        \n",
    "        energy = torch.bmm(self.key, self.query.unsqueeze(2))\n",
    "        \n",
    "        energy = torch.squeeze(energy, dim = 2)\n",
    "\n",
    "        #What should the mask least value be? \n",
    "        energy.masked_fill_(self.mask, -1e9)\n",
    "        \n",
    "        attention = torch.nn.functional.softmax(energy, dim = 1)\n",
    "        context = torch.bmm(torch.permute(self.value,[0,2,1]),attention.unsqueeze(2)).squeeze(2)\n",
    "        return context, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decoder ~ according to the speller of the LAS paper \n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embed_dim, projection, vocab_size, decoder_hidden_size, decoder_output_size, encoder_output_size):\n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        \"\"\"\n",
    "        A simple lookup table that stores embeddings of a fixed dictionary and size.\n",
    "        This module is often used to store word embeddings and retrieve them using indices. \n",
    "        The input to the module is a list of indices, and the output is the corresponding word embeddings.\n",
    "        \"\"\"\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx = 0 ).cuda()\n",
    "        self.lstm_cells = nn.Sequential(nn.LSTMCell(embed_dim + projection , decoder_hidden_size) , nn.LSTMCell(decoder_hidden_size , decoder_output_size))\n",
    "        self.vocab_size = vocab_size\n",
    "        self.attention = Attention(2*encoder_output_size , decoder_output_size, projection)\n",
    "        self.character_prob = nn.Linear(512, vocab_size)\n",
    "        self.device = \"cuda\"\n",
    "        \n",
    "    def forward(self, encoder_output, encoder_len, y = None, mode = \"train\", teacherForcingRate = 0.1, isGumbel = False ):\n",
    "       \n",
    "        # batch, key_seq_max_len, key_value_size = key.shape\n",
    "        batch, encoder_max_seq, _ = encoder_output.shape\n",
    "\n",
    "        # # Attention mask for making the system autoregressive \n",
    "        # mask = torch.arange(key_seq_max_len).unsqueeze(0)>=encoder_len.unsqueeze(1)\n",
    "        # mask = mask.to(self.device)\n",
    "\n",
    "        # List to store output attention plots \n",
    "        predictions, attention_plot = [], []\n",
    "        prediction = torch.full((batch,1), fill_value = 0 ,device= self.device)\n",
    "        \n",
    "        # Hidden states\n",
    "        hidden_states= [None]*len(self.lstm_cells)\n",
    "        self.attention.key_value_calc(encoder_output, encoder_len)\n",
    "        \n",
    "        context = self.attention.value[:,0,:]\n",
    "\n",
    "        if mode == \"train\":\n",
    "            max_len = y.shape[1]\n",
    "            char_embedding = self.embedding(y)\n",
    "        else: \n",
    "            max_len = 600\n",
    "\n",
    "        for i in range(max_len):\n",
    "            if mode == \"train\":\n",
    "                # Teacher Forcing regime ~ Assigned and picked randomly \n",
    "                teacher_forcing = True if random.random() > teacherForcingRate else False \n",
    "                if not teacher_forcing:\n",
    "                    if i != 0 : # use Gumbel noise to add noise to add variety to phoneme\n",
    "                        char_embed = torch.nn.functional.gumbel_softmax(prediction).mm(self.embedding.weight)\n",
    "                    else:\n",
    "                        char_embed = self.embedding(prediction.argmax(dim=-1))\n",
    "                else:\n",
    "                    if i == 0:\n",
    "                        char_embed = self.embedding(torch.zeros(batch, dtype = torch.long).fill_(VOCAB_MAP['<sos>']).to(self.device)) \n",
    "                    else: \n",
    "                        char_embed = char_embedding[:,i-1,:] # ground truth teacher forcing \n",
    "            # Validation mode \n",
    "            else: \n",
    "                if i == 0: \n",
    "                    char_embed = self.embedding(torch.zeros(batch, dtype = torch.long).fill_(VOCAB_MAP['<sos>']).to(self.device)) \n",
    "                else: \n",
    "                    char_embed = self.embedding(prediction.argmax(dim = -1)) # feed in the previous prediction as input \n",
    "            \n",
    "            # Input to the decoder (prev embedding + context from attention mechanism) \n",
    "            decoder_input_embedding = torch.cat([char_embed, context.squeeze(1)], dim = 1)\n",
    "           \n",
    "            for i in range(len(self.lstm_cells)):\n",
    "                # An LSTM Cell returns (h,c) -> h = hidden state, c = cell memory state\n",
    "                # Using 2 LSTM Cells is akin to a 2 layer LSTM looped through t timesteps \n",
    "                # The second LSTM Cell takes in the output hidden state of the first LSTM Cell (from the current timestep) as Input, along with the hidden and cell states of the cell from the previous timestep\n",
    "                hidden_states[i] = self.lstm_cells[i](decoder_input_embedding, hidden_states[i]) \n",
    "                decoder_input_embedding = hidden_states[i][0]\n",
    "\n",
    "            decoder_output_embedding = hidden_states[-1][0]\n",
    "            # What is the query? (same len as the key)\n",
    "            # Hidden state of the LSTM \n",
    "            # 8x768 and 128x30\n",
    "            # decoder_output_embeddings, mask\n",
    "            context, attention = self.attention(decoder_output_embedding)\n",
    "            attention_plot.append(attention[0].detach().cpu())\n",
    "            \n",
    "            output_context = torch.cat([self.attention.query, context], dim = 1)\n",
    "            prediction = self.character_prob(output_context)\n",
    "            predictions.append(prediction.unsqueeze(1))\n",
    "        attentions = torch.stack(attention_plot, dim = 0)\n",
    "        predictions = torch.cat(predictions, dim = 1 )\n",
    "\n",
    "        return predictions, attentions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Combining the pipelines of the Seq2Seq model\n",
    "\"\"\"\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, input_size, encoder_hidden_size, vocab_size, embed_size, decoder_hidden_size, decoder_output_size, projection_size = 128 ):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        \"\"\"\n",
    "        Parameters of each of the model classes \n",
    "        Encoder : input_size, encoder_hidden_size\n",
    "        Decoder : embed_dim, projection, vocab_size, decoder_hidden_size, decoder_output_size, encoder_output_size\n",
    "        \"\"\"\n",
    "\n",
    "        self.encoder =  Encoder(input_size = input_size, encoder_hidden_size = encoder_hidden_size)\n",
    "        #self.attention = Attention(encoder_output_size= 2*encoder_hidden_size, decoder_output_size=decoder_output_size, projection= projection_size)\n",
    "        self.decoder =  Decoder(embed_dim = embed_size, projection = projection_size, vocab_size = vocab_size, decoder_hidden_size = decoder_hidden_size, decoder_output_size=decoder_output_size,encoder_output_size = encoder_hidden_size)\n",
    "    def forward(self, x, x_lens, y = None, mode = \"none\"):\n",
    "        \n",
    "        encoder_outputs, encoder_lens = self.encoder(x, x_lens)\n",
    "        # encoder_output, key, value, encoder_len, y = None, mode = \"train\", teacherForcingRate = 0.1, isGumbel = False \n",
    "        predictions, attention_map = self.decoder(encoder_outputs, encoder_lens , y, mode = mode)\n",
    "\n",
    "        return predictions, attention_map \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda\\envs\\PointNet\\lib\\site-packages\\torch\\nn\\modules\\rnn.py:62: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.1 and num_layers=1\n",
      "  warnings.warn(\"dropout option adds dropout after all but last \"\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (base_lstm): LSTM(15, 512, batch_first=True, dropout=0.1, bidirectional=True)\n",
       "    (pblstm): Sequential(\n",
       "      (0): PBLSTM(\n",
       "        (blstm): LSTM(2048, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "      )\n",
       "      (1): LockedDropout()\n",
       "      (2): PBLSTM(\n",
       "        (blstm): LSTM(2048, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "      )\n",
       "      (3): LockedDropout()\n",
       "      (4): PBLSTM(\n",
       "        (blstm): LSTM(2048, 512, num_layers=2, batch_first=True, dropout=0.3, bidirectional=True)\n",
       "      )\n",
       "      (5): LockedDropout()\n",
       "    )\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(30, 512, padding_idx=0)\n",
       "    (lstm_cells): Sequential(\n",
       "      (0): LSTMCell(768, 512)\n",
       "      (1): LSTMCell(512, 128)\n",
       "    )\n",
       "    (attention): Attention(\n",
       "      (key_layer): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (value_layer): Linear(in_features=1024, out_features=256, bias=True)\n",
       "      (query_layer): Linear(in_features=128, out_features=256, bias=True)\n",
       "    )\n",
       "    (character_prob): Linear(in_features=512, out_features=30, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Model initialization \n",
    "\"\"\"\n",
    "DEVICE = \"cuda\"\n",
    "model  = Seq2Seq(input_size=15,encoder_hidden_size=512,vocab_size=len(VOCAB),\n",
    "            embed_size=512,decoder_hidden_size=512,decoder_output_size=128,projection_size=256)\n",
    "model.to(DEVICE)\n",
    "# print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We have given you this utility function which takes a sequence of indices and converts them to a list of characters\n",
    "def indices_to_chars(indices, vocab):\n",
    "    tokens = []\n",
    "    for i in indices: # This loops through all the indices\n",
    "        if vocab[int(i)] == vocab[SOS_TOKEN]: # If SOS is encountered, dont add it to the final list\n",
    "            continue\n",
    "        elif vocab[int(i)] == vocab[EOS_TOKEN]: # If EOS is encountered, stop the decoding process\n",
    "            break\n",
    "        else:\n",
    "            tokens.append(vocab[i])\n",
    "    return tokens\n",
    "\n",
    "# To make your life more easier, we have given the Levenshtein distantce / Edit distance calculation code\n",
    "def calc_edit_distance(predictions, y, ly, vocab= VOCAB, print_example= False):\n",
    "\n",
    "    dist                = 0\n",
    "    batch_size, seq_len = predictions.shape\n",
    "\n",
    "    for batch_idx in range(batch_size): \n",
    "\n",
    "        y_sliced    = indices_to_chars(y[batch_idx,0:ly[batch_idx]], vocab)\n",
    "        pred_sliced = indices_to_chars(predictions[batch_idx], vocab)\n",
    "\n",
    "        # Strings - When you are using characters from the AudioDataset\n",
    "        y_string    = ''.join(y_sliced)\n",
    "        pred_string = ''.join(pred_sliced)\n",
    "        \n",
    "        dist        += Levenshtein.distance(pred_string, y_string)\n",
    "        # Comment the above abd uncomment below for toy dataset \n",
    "        # dist      += Levenshtein.distance(y_sliced, pred_sliced)\n",
    "\n",
    "    if print_example: \n",
    "        # Print y_sliced and pred_sliced if you are using the toy dataset\n",
    "        # print(\"Ground Truth : \", y_string)\n",
    "        # print(\"Prediction   : \", pred_string)\n",
    "        print(\"Ground Truth : \", y_sliced)\n",
    "        print(\"Prediction   : \", pred_sliced)\n",
    "        \n",
    "    dist/=batch_size\n",
    "    return dist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper-parameters under consideration \n",
    "- Optimizer\n",
    "    - Learning Rate\n",
    "    - Weight Decay\n",
    "- Learning Rate Scheduler\n",
    "    - reduction \n",
    "- Loss function \n",
    "    - Reduction\n",
    "    - Factor \n",
    "    - Patience "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(),lr = 2e-3, weight_decay = 5e-6 )\n",
    "criterion = nn.CrossEntropyLoss(reduction = \"none\")\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min', factor = 0.4, patience = 2)\n",
    "scaler = torch.cuda.amp.GradScaler()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train and evaluate "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/100 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train:   0%|          | 0/13 [00:00<?, ?it/s]C:\\Users\\thopa\\AppData\\Local\\Temp\\ipykernel_12064\\2177239868.py:14: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
      "  x_lens = x_lens//2\n",
      "C:\\Users\\thopa\\AppData\\Local\\Temp\\ipykernel_12064\\1372442772.py:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  max_len = torch.max(torch.tensor(y_len))\n",
      "                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth :  ['W', 'H', 'A', 'T', ' ', 'A', 'L', 'T', 'E', 'R', 'N', 'A', 'T', 'I', 'V', 'E', ' ', 'W', 'A', 'S', ' ', 'T', 'H', 'E', 'R', 'E', ' ', 'F', 'O', 'R', ' ', 'H', 'E', 'R']\n",
      "Prediction   :  ['H', 'E', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A']\n",
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 1/100 [00:48<1:19:15, 48.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth :  ['W', 'H', 'A', 'T', ' ', 'A', 'L', 'T', 'E', 'R', 'N', 'A', 'T', 'I', 'V', 'E', ' ', 'W', 'A', 'S', ' ', 'T', 'H', 'E', 'R', 'E', ' ', 'F', 'O', 'R', ' ', 'H', 'E', 'R']\n",
      "Prediction   :  ['H', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'A', 'N', 'D', ' ']\n",
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 2/100 [01:36<1:18:55, 48.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:  69%|██████▉   | 9/13 [00:11<00:05,  1.32s/it, dist=553.9583]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth :  ['O', ' ', 'I', 'F', ' ', 'Y', 'O', 'U', ' ', 'P', 'L', 'A', 'Y', ' ', 'U', 'S', ' ', 'A', ' ', 'R', 'O', 'U', 'N', 'D', 'E', 'L', ' ', 'S', 'I', 'N', 'G', 'E', 'R', ' ', 'H', 'O', 'W', ' ', 'C', 'A', 'N', ' ', 'T', 'H', 'A', 'T', ' ', 'H', 'A', 'R', 'M', ' ', 'T', 'H', 'E', ' ', 'E', 'M', 'P', 'E', 'R', 'O', 'R', \"'\", 'S', ' ', 'D', 'A', 'U', 'G', 'H', 'T', 'E', 'R']\n",
      "Prediction   :  ['T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ', 'T', 'H', 'E', ' ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 3/100 [02:24<1:17:51, 48.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:  54%|█████▍    | 7/13 [00:09<00:08,  1.34s/it, dist=540.2679]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth :  ['I', ' ', 'S', 'A', 'W', ' ', 'T', 'H', 'E', ' ', 'L', 'A', 'D', 'Y', ' ', 'W', 'H', 'O', ' ', 'E', 'R', 'E', 'W', 'H', 'I', 'L', 'E', ' ', 'A', 'P', 'P', 'E', 'A', 'R', 'E', 'D', ' ', 'V', 'E', 'I', 'L', 'E', 'D', ' ', 'U', 'N', 'D', 'E', 'R', 'N', 'E', 'A', 'T', 'H', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'G', 'E', 'L', 'I', 'C', ' ', 'F', 'E', 'S', 'T', 'I', 'V', 'A', 'L', ' ', 'D', 'I', 'R', 'E', 'C', 'T', ' ', 'H', 'E', 'R', ' ', 'E', 'Y', 'E', 'S', ' ', 'T', 'O', ' ', 'M', 'E', ' ', 'A', 'C', 'R', 'O', 'S', 'S', ' ', 'T', 'H', 'E', ' ', 'R', 'I', 'V', 'E', 'R']\n",
      "Prediction   :  ['H', 'A', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 4/100 [03:12<1:17:05, 48.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:  31%|███       | 4/13 [00:05<00:11,  1.27s/it, dist=559.8750]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth :  ['T', 'H', 'O', 'U', ' ', 'M', 'A', 'K', 'E', 'S', 'T', ' ', 'M', 'E', ' ', 'R', 'E', 'M', 'E', 'M', 'B', 'E', 'R', ' ', 'W', 'H', 'E', 'R', 'E', ' ', 'A', 'N', 'D', ' ', 'W', 'H', 'A', 'T', ' ', 'P', 'R', 'O', 'S', 'E', 'R', 'P', 'I', 'N', 'A', ' ', 'T', 'H', 'A', 'T', ' ', 'M', 'O', 'M', 'E', 'N', 'T', ' ', 'W', 'A', 'S', ' ', 'W', 'H', 'E', 'N', ' ', 'L', 'O', 'S', 'T', ' ', 'H', 'E', 'R', ' ', 'M', 'O', 'T', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'S', 'H', 'E', ' ', 'H', 'E', 'R', 'S', 'E', 'L', 'F', ' ', 'T', 'H', 'E', ' ', 'S', 'P', 'R', 'I', 'N', 'G']\n",
      "Prediction   :  ['H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'H', 'E', 'R']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 5/100 [04:00<1:15:55, 47.95s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:  62%|██████▏   | 8/13 [00:10<00:07,  1.42s/it, dist=546.4375]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth :  ['W', 'H', 'E', 'N', 'C', 'E', ' ', 'S', 'H', 'E', ' ', 'T', 'O', ' ', 'M', 'E', ' ', 'I', 'N', ' ', 'T', 'H', 'O', 'S', 'E', ' ', 'D', 'E', 'S', 'I', 'R', 'E', 'S', ' ', 'O', 'F', ' ', 'M', 'I', 'N', 'E', ' ', 'W', 'H', 'I', 'C', 'H', ' ', 'L', 'E', 'D', ' ', 'T', 'H', 'E', 'E', ' ', 'T', 'O', ' ', 'T', 'H', 'E', ' ', 'L', 'O', 'V', 'I', 'N', 'G', ' ', 'O', 'F', ' ', 'T', 'H', 'A', 'T', ' ', 'G', 'O', 'O', 'D', ' ', 'B', 'E', 'Y', 'O', 'N', 'D', ' ', 'W', 'H', 'I', 'C', 'H', ' ', 'T', 'H', 'E', 'R', 'E', ' ', 'I', 'S', ' ', 'N', 'O', 'T', 'H', 'I', 'N', 'G', ' ', 'T', 'O', ' ', 'A', 'S', 'P', 'I', 'R', 'E', ' ', 'T', 'O']\n",
      "Prediction   :  ['T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'H', 'E', 'R']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 6/100 [04:47<1:14:39, 47.65s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 7/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:  38%|███▊      | 5/13 [00:06<00:10,  1.25s/it, dist=541.6250]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth :  ['N', 'O', 'R', ' ', 'E', 'V', 'E', 'N', ' ', 'T', 'H', 'U', 'S', ' ', 'O', 'U', 'R', ' ', 'W', 'A', 'Y', ' ', 'C', 'O', 'N', 'T', 'I', 'N', 'U', 'E', 'D', ' ', 'F', 'A', 'R', ' ', 'B', 'E', 'F', 'O', 'R', 'E', ' ', 'T', 'H', 'E', ' ', 'L', 'A', 'D', 'Y', ' ', 'W', 'H', 'O', 'L', 'L', 'Y', ' ', 'T', 'U', 'R', 'N', 'E', 'D', ' ', 'H', 'E', 'R', 'S', 'E', 'L', 'F', ' ', 'U', 'N', 'T', 'O', ' ', 'M', 'E', ' ', 'S', 'A', 'Y', 'I', 'N', 'G', ' ', 'B', 'R', 'O', 'T', 'H', 'E', 'R', ' ', 'L', 'O', 'O', 'K', ' ', 'A', 'N', 'D', ' ', 'L', 'I', 'S', 'T', 'E', 'N']\n",
      "Prediction   :  ['T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'R', 'O', 'V', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 7/100 [05:35<1:14:09, 47.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 8/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:  77%|███████▋  | 10/13 [00:13<00:04,  1.34s/it, dist=523.2375]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth :  ['T', 'H', 'E', ' ', 'L', 'A', 'D', 'I', 'E', 'S']\n",
      "Prediction   :  ['T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'M', 'O', 'R', 'L', 'A', 'N', 'D']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                   \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  8%|▊         | 8/100 [06:23<1:13:23, 47.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:  31%|███       | 4/13 [00:05<00:11,  1.24s/it, dist=539.8750]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth :  ['T', 'H', 'O', 'U', ' ', 'M', 'A', 'K', 'E', 'S', 'T', ' ', 'M', 'E', ' ', 'R', 'E', 'M', 'E', 'M', 'B', 'E', 'R', ' ', 'W', 'H', 'E', 'R', 'E', ' ', 'A', 'N', 'D', ' ', 'W', 'H', 'A', 'T', ' ', 'P', 'R', 'O', 'S', 'E', 'R', 'P', 'I', 'N', 'A', ' ', 'T', 'H', 'A', 'T', ' ', 'M', 'O', 'M', 'E', 'N', 'T', ' ', 'W', 'A', 'S', ' ', 'W', 'H', 'E', 'N', ' ', 'L', 'O', 'S', 'T', ' ', 'H', 'E', 'R', ' ', 'M', 'O', 'T', 'H', 'E', 'R', ' ', 'H', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'S', 'H', 'E', ' ', 'H', 'E', 'R', 'S', 'E', 'L', 'F', ' ', 'T', 'H', 'E', ' ', 'S', 'P', 'R', 'I', 'N', 'G']\n",
      "Prediction   :  ['H', 'A', 'D', ' ', 'N', 'O', 'T', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'W', 'A', 'S', ' ', 'A', 'N', 'D']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  9%|▉         | 9/100 [07:10<1:12:14, 47.63s/it]                  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 10/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:  92%|█████████▏| 12/13 [00:16<00:01,  1.49s/it, dist=542.3333]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth :  ['I', 'F', ' ', 'W', 'E', ' ', 'H', 'A', 'D', ' ', 'B', 'E', 'E', 'N', ' ', 'B', 'R', 'O', 'T', 'H', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'S', 'I', 'S', 'T', 'E', 'R', ' ', 'I', 'N', 'D', 'E', 'E', 'D', ' ', 'T', 'H', 'E', 'R', 'E', ' ', 'W', 'A', 'S', ' ', 'N', 'O', 'T', 'H', 'I', 'N', 'G']\n",
      "Prediction   :  ['A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 10/100 [07:57<1:11:08, 47.43s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 11/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:  85%|████████▍ | 11/13 [00:15<00:02,  1.42s/it, dist=525.2273]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth :  ['I', 'T', ' ', 'I', 'S', ' ', 'T', 'H', 'E', ' ', 'E', 'X', 'P', 'R', 'E', 'S', 'S', 'I', 'O', 'N', ' ', 'O', 'F', ' ', 'L', 'I', 'F', 'E', ' ', 'U', 'N', 'D', 'E', 'R', ' ', 'C', 'R', 'U', 'D', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'M', 'O', 'R', 'E', ' ', 'R', 'I', 'G', 'I', 'D', ' ', 'C', 'O', 'N', 'D', 'I', 'T', 'I', 'O', 'N', 'S', ' ', 'T', 'H', 'A', 'N', ' ', 'O', 'U', 'R', 'S', ' ', 'L', 'I', 'V', 'E', 'D', ' ', 'B', 'Y', ' ', 'P', 'E', 'O', 'P', 'L', 'E', ' ', 'W', 'H', 'O', ' ', 'L', 'O', 'V', 'E', 'D', ' ', 'A', 'N', 'D', ' ', 'H', 'A', 'T', 'E', 'D', ' ', 'M', 'O', 'R', 'E', ' ', 'N', 'A', 'I', 'V', 'E', 'L', 'Y', ' ', 'A', 'G', 'E', 'D', ' ', 'S', 'O', 'O', 'N', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'D', 'I', 'E', 'D', ' ', 'Y', 'O', 'U', 'N', 'G', 'E', 'R', ' ', 'T', 'H', 'A', 'N', ' ', 'W', 'E', ' ', 'D', 'O']\n",
      "Prediction   :  ['A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█         | 11/100 [08:45<1:10:21, 47.44s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 12/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:  38%|███▊      | 5/13 [00:06<00:10,  1.28s/it, dist=537.6500]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth :  ['N', 'O', 'R', ' ', 'E', 'V', 'E', 'N', ' ', 'T', 'H', 'U', 'S', ' ', 'O', 'U', 'R', ' ', 'W', 'A', 'Y', ' ', 'C', 'O', 'N', 'T', 'I', 'N', 'U', 'E', 'D', ' ', 'F', 'A', 'R', ' ', 'B', 'E', 'F', 'O', 'R', 'E', ' ', 'T', 'H', 'E', ' ', 'L', 'A', 'D', 'Y', ' ', 'W', 'H', 'O', 'L', 'L', 'Y', ' ', 'T', 'U', 'R', 'N', 'E', 'D', ' ', 'H', 'E', 'R', 'S', 'E', 'L', 'F', ' ', 'U', 'N', 'T', 'O', ' ', 'M', 'E', ' ', 'S', 'A', 'Y', 'I', 'N', 'G', ' ', 'B', 'R', 'O', 'T', 'H', 'E', 'R', ' ', 'L', 'O', 'O', 'K', ' ', 'A', 'N', 'D', ' ', 'L', 'I', 'S', 'T', 'E', 'N']\n",
      "Prediction   :  ['T', 'H', 'E', ' ', 'C', 'O', 'U', 'L', 'D', ' ', 'N', 'O', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'A']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 12%|█▏        | 12/100 [09:33<1:09:48, 47.60s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 13/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:  85%|████████▍ | 11/13 [00:15<00:02,  1.45s/it, dist=537.4318]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth :  ['I', 'T', ' ', 'I', 'S', ' ', 'T', 'H', 'E', ' ', 'E', 'X', 'P', 'R', 'E', 'S', 'S', 'I', 'O', 'N', ' ', 'O', 'F', ' ', 'L', 'I', 'F', 'E', ' ', 'U', 'N', 'D', 'E', 'R', ' ', 'C', 'R', 'U', 'D', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'M', 'O', 'R', 'E', ' ', 'R', 'I', 'G', 'I', 'D', ' ', 'C', 'O', 'N', 'D', 'I', 'T', 'I', 'O', 'N', 'S', ' ', 'T', 'H', 'A', 'N', ' ', 'O', 'U', 'R', 'S', ' ', 'L', 'I', 'V', 'E', 'D', ' ', 'B', 'Y', ' ', 'P', 'E', 'O', 'P', 'L', 'E', ' ', 'W', 'H', 'O', ' ', 'L', 'O', 'V', 'E', 'D', ' ', 'A', 'N', 'D', ' ', 'H', 'A', 'T', 'E', 'D', ' ', 'M', 'O', 'R', 'E', ' ', 'N', 'A', 'I', 'V', 'E', 'L', 'Y', ' ', 'A', 'G', 'E', 'D', ' ', 'S', 'O', 'O', 'N', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'D', 'I', 'E', 'D', ' ', 'Y', 'O', 'U', 'N', 'G', 'E', 'R', ' ', 'T', 'H', 'A', 'N', ' ', 'W', 'E', ' ', 'D', 'O']\n",
      "Prediction   :  ['A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'T', 'O', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', ' ', 'T', 'H', 'E', ' ']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 13%|█▎        | 13/100 [10:21<1:09:16, 47.78s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 14/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:  38%|███▊      | 5/13 [00:06<00:10,  1.25s/it, dist=534.3250]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth :  ['N', 'O', 'R', ' ', 'E', 'V', 'E', 'N', ' ', 'T', 'H', 'U', 'S', ' ', 'O', 'U', 'R', ' ', 'W', 'A', 'Y', ' ', 'C', 'O', 'N', 'T', 'I', 'N', 'U', 'E', 'D', ' ', 'F', 'A', 'R', ' ', 'B', 'E', 'F', 'O', 'R', 'E', ' ', 'T', 'H', 'E', ' ', 'L', 'A', 'D', 'Y', ' ', 'W', 'H', 'O', 'L', 'L', 'Y', ' ', 'T', 'U', 'R', 'N', 'E', 'D', ' ', 'H', 'E', 'R', 'S', 'E', 'L', 'F', ' ', 'U', 'N', 'T', 'O', ' ', 'M', 'E', ' ', 'S', 'A', 'Y', 'I', 'N', 'G', ' ', 'B', 'R', 'O', 'T', 'H', 'E', 'R', ' ', 'L', 'O', 'O', 'K', ' ', 'A', 'N', 'D', ' ', 'L', 'I', 'S', 'T', 'E', 'N']\n",
      "Prediction   :  ['A', 'N', 'D', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'R', 'E', 'A', 'T', 'E', 'R', ' ', 'A', 'N', 'D']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 14%|█▍        | 14/100 [11:08<1:08:14, 47.61s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 15/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:  92%|█████████▏| 12/13 [00:17<00:01,  1.55s/it, dist=525.3542]               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth :  ['I', 'F', ' ', 'W', 'E', ' ', 'H', 'A', 'D', ' ', 'B', 'E', 'E', 'N', ' ', 'B', 'R', 'O', 'T', 'H', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'S', 'I', 'S', 'T', 'E', 'R', ' ', 'I', 'N', 'D', 'E', 'E', 'D', ' ', 'T', 'H', 'E', 'R', 'E', ' ', 'W', 'A', 'S', ' ', 'N', 'O', 'T', 'H', 'I', 'N', 'G']\n",
      "Prediction   :  ['A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'F', 'A', 'T', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 15/100 [11:56<1:07:27, 47.62s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 16/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:  54%|█████▍    | 7/13 [00:09<00:08,  1.49s/it, dist=525.2143]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth :  ['I', ' ', 'S', 'A', 'W', ' ', 'T', 'H', 'E', ' ', 'L', 'A', 'D', 'Y', ' ', 'W', 'H', 'O', ' ', 'E', 'R', 'E', 'W', 'H', 'I', 'L', 'E', ' ', 'A', 'P', 'P', 'E', 'A', 'R', 'E', 'D', ' ', 'V', 'E', 'I', 'L', 'E', 'D', ' ', 'U', 'N', 'D', 'E', 'R', 'N', 'E', 'A', 'T', 'H', ' ', 'T', 'H', 'E', ' ', 'A', 'N', 'G', 'E', 'L', 'I', 'C', ' ', 'F', 'E', 'S', 'T', 'I', 'V', 'A', 'L', ' ', 'D', 'I', 'R', 'E', 'C', 'T', ' ', 'H', 'E', 'R', ' ', 'E', 'Y', 'E', 'S', ' ', 'T', 'O', ' ', 'M', 'E', ' ', 'A', 'C', 'R', 'O', 'S', 'S', ' ', 'T', 'H', 'E', ' ', 'R', 'I', 'V', 'E', 'R']\n",
      "Prediction   :  ['A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 16%|█▌        | 16/100 [12:43<1:06:29, 47.49s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 17/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Val:  15%|█▌        | 2/13 [00:02<00:14,  1.35s/it, dist=546.1875]                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ground Truth :  ['B', 'U', 'T', ' ', 'C', 'A', 'N', ' ', 'H', 'E', ' ', 'U', 'N', 'D', 'E', 'R', 'S', 'T', 'A', 'N', 'D', ' ', 'Y', 'O', 'U', ' ', 'Y', 'E', 'S']\n",
      "Prediction   :  ['A', 'N', 'D', ' ', 'H', 'E', 'R', ' ', 'F', 'A', 'T', 'H', 'E', 'R', ' ', 'T', 'H', 'E', ' ', 'G', 'E', 'N', 'E', 'R', 'A', 'L', ' ', 'S', 'E', 'E', 'N', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E', ' ', 'P', 'E', 'R', 'S', 'E', 'N', 'T', ' ', 'A', 'N', 'D', ' ', 'T', 'H', 'E']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [13:31<1:05:55, 47.66s/it]                 "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 18/100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 17%|█▋        | 17/100 [14:10<1:09:14, 50.05s/it], dist=522.1429]                \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [17], line 52\u001b[0m\n\u001b[0;32m     50\u001b[0m x,y,lx,ly \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mto(DEVICE), y\u001b[39m.\u001b[39mto(DEVICE), lx, ly\n\u001b[0;32m     51\u001b[0m \u001b[39mwith\u001b[39;00m torch\u001b[39m.\u001b[39minference_mode():\n\u001b[1;32m---> 52\u001b[0m     predictions, attention \u001b[39m=\u001b[39m model(x, lx, y \u001b[39m=\u001b[39;49m \u001b[39mNone\u001b[39;49;00m)\n\u001b[0;32m     53\u001b[0m greedy_predictions \u001b[39m=\u001b[39m predictions\u001b[39m.\u001b[39margmax(dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     55\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m==\u001b[39m debug_ind: \n",
      "File \u001b[1;32md:\\anaconda\\envs\\PointNet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [12], line 18\u001b[0m, in \u001b[0;36mSeq2Seq.forward\u001b[1;34m(self, x, x_lens, y, mode)\u001b[0m\n\u001b[0;32m     16\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x, x_lens, y \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m, mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mnone\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[1;32m---> 18\u001b[0m     encoder_outputs, encoder_lens \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(x, x_lens)\n\u001b[0;32m     19\u001b[0m     \u001b[39m# encoder_output, key, value, encoder_len, y = None, mode = \"train\", teacherForcingRate = 0.1, isGumbel = False \u001b[39;00m\n\u001b[0;32m     20\u001b[0m     predictions, attention_map \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecoder(encoder_outputs, encoder_lens , y, mode \u001b[39m=\u001b[39m mode)\n",
      "File \u001b[1;32md:\\anaconda\\envs\\PointNet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [8], line 27\u001b[0m, in \u001b[0;36mEncoder.forward\u001b[1;34m(self, x, x_lens)\u001b[0m\n\u001b[0;32m     25\u001b[0m \u001b[39m#print(type(pack_padd_out))\u001b[39;00m\n\u001b[0;32m     26\u001b[0m out_lstm, _  \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbase_lstm(pack_padd_out)\n\u001b[1;32m---> 27\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpblstm(out_lstm)\n\u001b[0;32m     28\u001b[0m encoder_outputs, encoder_lens \u001b[39m=\u001b[39m pad_packed_sequence(encoder_outputs, batch_first\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[0;32m     30\u001b[0m \u001b[39mreturn\u001b[39;00m encoder_outputs, encoder_lens\n",
      "File \u001b[1;32md:\\anaconda\\envs\\PointNet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32md:\\anaconda\\envs\\PointNet\\lib\\site-packages\\torch\\nn\\modules\\container.py:139\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    137\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    138\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 139\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    140\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32md:\\anaconda\\envs\\PointNet\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1131\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn [7], line 7\u001b[0m, in \u001b[0;36mLockedDropout.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, x):\n\u001b[1;32m----> 7\u001b[0m     output, output_len \u001b[39m=\u001b[39m pad_packed_sequence(x, batch_first\u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m      8\u001b[0m     x \u001b[39m=\u001b[39m output\n\u001b[0;32m      9\u001b[0m     x \u001b[39m=\u001b[39m x\u001b[39m.\u001b[39mclone()\n",
      "File \u001b[1;32md:\\anaconda\\envs\\PointNet\\lib\\site-packages\\torch\\nn\\utils\\rnn.py:334\u001b[0m, in \u001b[0;36mpad_packed_sequence\u001b[1;34m(sequence, batch_first, padding_value, total_length)\u001b[0m\n\u001b[0;32m    332\u001b[0m \u001b[39mif\u001b[39;00m unsorted_indices \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    333\u001b[0m     batch_dim \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m \u001b[39mif\u001b[39;00m batch_first \u001b[39melse\u001b[39;00m \u001b[39m1\u001b[39m\n\u001b[1;32m--> 334\u001b[0m     \u001b[39mreturn\u001b[39;00m padded_output\u001b[39m.\u001b[39mindex_select(batch_dim, unsorted_indices), lengths[unsorted_indices]\n\u001b[0;32m    335\u001b[0m \u001b[39mreturn\u001b[39;00m padded_output, lengths\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "epochs = 100 \n",
    "best_lev_dist = float(\"inf\")\n",
    "tf_rate = 0.5\n",
    "for epoch in tqdm(range(epochs)): \n",
    "    print(\"\\nEpoch: {}/{}\".format(epoch+1, epochs))\n",
    "    curr_lr = float(optimizer.param_groups[0]['lr'])\n",
    "\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    batch_bar = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
    "    running_loss = 0\n",
    "\n",
    "    # Levenstein distance debug at random indices \n",
    "    \n",
    "    for i,(x, y, x_len, y_len) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()\n",
    "        x, x_len, y, y_len = x.to(DEVICE), x_len, y.to(DEVICE), y_len\n",
    "        pred, attn = model(x = x, x_lens = x_len, y = y, mode = \"train\")\n",
    "    \n",
    "\n",
    "        max_len = torch.max(torch.tensor(y_len))\n",
    "        lst = torch.arange(0,max_len).repeat(y_len.size(0),1)\n",
    "\n",
    "        seq_len = y_len.unsqueeze(1).expand(y_len.size(0),max_len)\n",
    "        mask = (lst<seq_len).int().cuda() \n",
    "        loss = criterion(pred.view(-1, pred.size(2)), y.view(-1))\n",
    "        masked_loss = torch.sum(loss * mask.view(-1)) / torch.sum(mask)\n",
    "\n",
    "        masked_loss.backward()\n",
    "        running_loss+=masked_loss\n",
    "        optimizer.step()\n",
    "        idx = \"{}_{}\".format(epoch, i)\n",
    "        torch.cuda.empty_cache()\n",
    "        batch_bar.set_postfix(\n",
    "            loss=\"{:.04f}\".format(running_loss/(i+1)),\n",
    "            lr=\"{:.08f}\".format(float(optimizer.param_groups[0]['lr'])))\n",
    "        batch_bar.update()\n",
    "\n",
    "        del x, y, x_len, y_len\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    # Validate \n",
    "    model.eval()\n",
    "    batch_bar = tqdm(total = len(val_loader), dynamic_ncols= True, position = 0 ,leave = False, desc = \"Val\")\n",
    "    debug_ind = np.random.randint(0,len(val_loader))\n",
    "    running_lev_dist = 0.0\n",
    "\n",
    "    for i, (x,y,lx,ly) in enumerate(val_loader):\n",
    "        x,y,lx,ly = x.to(DEVICE), y.to(DEVICE), lx, ly\n",
    "        with torch.inference_mode():\n",
    "            predictions, attention = model(x, lx, y = None)\n",
    "        greedy_predictions = predictions.argmax(dim = -1)\n",
    "\n",
    "        if i == debug_ind: \n",
    "            running_lev_dist += calc_edit_distance(greedy_predictions, y ,ly, VOCAB, print_example = True)\n",
    "        else: \n",
    "            running_lev_dist += calc_edit_distance(greedy_predictions, y ,ly, VOCAB, print_example = False)\n",
    "        \n",
    "        batch_bar.set_postfix(\n",
    "            dist=\"{:.04f}\".format(running_lev_dist/(i+1)))\n",
    "        batch_bar.update()\n",
    "        del x, y, lx, ly\n",
    "        torch.cuda.empty_cache()\n",
    "    batch_bar.close()\n",
    "    running_lev_dist /= len(val_loader)\n",
    "    val_dist = running_lev_dist\n",
    "\n",
    "    #wandb.log({\"train loss\": running_loss, \"lev_dist\": val_dist, \"tf_rate\": tf_rate, \"lr\":curr_lr})\n",
    "    scheduler.step(val_dist)\n",
    "\n",
    "    if val_dist <= best_lev_dist:\n",
    "        best_lev_dist = val_dist\n",
    "        print(\"Saving model\")\n",
    "        torch.save({'model_state_dict':model.state_dict(),\n",
    "                    'optimizer_state_dict':optimizer.state_dict(),\n",
    "                    'val_dist': val_dist, \n",
    "                    'epoch': epoch}, 'C:\\\\Users\\\\thopa\\\\Desktop\\\\Assignments\\\\11685\\\\HW4\\\\2022Implementation\\\\complete_implementation\\\\ckpt\\\\seq2seq_{}.pth'.format(epoch))\n",
    "        if val_dist<28:\n",
    "            tf_rate = tf_rate - 0.05*tf_rate\n",
    "            tf_rate = max(0.5, tf_rate)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PointNet",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "be71f7ebca6fc406e8f2b7eb97fc71d65ed284e191505e45141d8f3008802363"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
